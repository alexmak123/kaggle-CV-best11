{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df9024-56d1-44bd-b2af-73790793c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "PATH_DATA = \"/home/a.makarchuk@rit.va/Desktop/kaggle-CV-best11/dogs_vs_cats/data/\"\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_RATIO = 0.85\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26222e9c-95d8-4dfe-b672-53b9b5316162",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PATH_DATA)\n",
    "\n",
    "submission_df = pd.read_csv(f\"{PATH_DATA}sampleSubmission.csv\")\n",
    "list_train_imgs = os.listdir(f\"{PATH_DATA}train\")\n",
    "list_test_imgs = os.listdir(f\"{PATH_DATA}test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419b92c-d4c4-4d4a-91bb-b82d1921aed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHOW DATA\n",
    "\n",
    "rand_img_train_path = f\"{PATH_DATA}/train/{random.choice(list_train_imgs)}\"\n",
    "img = plt.imread(rand_img_train_path)\n",
    "img = cv2.resize(img, IMAGE_SIZE, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "print(rand_img_train_path)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b3a99-2e3a-47c2-ae07-5ded60f99b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_transform_img(path_img, image_size):\n",
    "    img_name = path_img.split(\"/\")[-1]\n",
    "    label = 1 if img_name.split(\".\")[0] == \"dog\" else 0\n",
    "    img = plt.imread(path_img)\n",
    "    img = cv2.resize(img, image_size, interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    return torch.from_numpy(img).float(), torch.from_numpy(np.array(label)).float()\n",
    "\n",
    "\n",
    "def generate_batch(list_imgs_path, batch_size):\n",
    "    curr_id = 0\n",
    "    while curr_id + batch_size < len(list_imgs_path):\n",
    "        images_tensor_list = list()\n",
    "        labels_tensor_list = list()\n",
    "        for i in range(batch_size):\n",
    "            img_tensor, label = load_and_transform_img(\n",
    "                PATH_DATA + \"train/\" + list_imgs_path[curr_id + i], IMAGE_SIZE\n",
    "            )\n",
    "            images_tensor_list.append(img_tensor)\n",
    "            labels_tensor_list.append(label)\n",
    "        yield torch.stack(images_tensor_list), torch.stack(labels_tensor_list).unsqueeze(-1)\n",
    "        curr_id += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c9e0e-669d-4d2b-923a-96335e8ffa22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CHECK LOADER\n",
    "\n",
    "\n",
    "for i, (stacked_imgs, stacked_lbls) in enumerate(generate_batch(list_train_imgs, 1)):\n",
    "    img = stacked_imgs.squeeze().numpy()\n",
    "    label = stacked_lbls.squeeze().numpy()\n",
    "    img = (img * 255).astype(int)\n",
    "    print(label)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76851213-ff24-47d6-a2dd-4483784757be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, image_size=IMAGE_SIZE):\n",
    "        super().__init__()\n",
    "        h, w = image_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.BatchNorm1d(h * w * 3),\n",
    "            nn.Linear(h * w * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33a7cb-df29-4c53-9090-3b1517792c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(DEVICE)\n",
    "sigmoid = nn.Sigmoid()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "list_imgs = list_train_imgs[:]\n",
    "random.shuffle(list_imgs)\n",
    "list_train_imgs = list_imgs[: int(len(list_imgs) * TRAIN_RATIO)]\n",
    "list_val_imgs = list_imgs[int(len(list_imgs) * TRAIN_RATIO) :]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    model.train()\n",
    "    for idx, (batch_imgs, labels) in tqdm(\n",
    "        enumerate(generate_batch(list_train_imgs, BATCH_SIZE)),\n",
    "        total=len(list_train_imgs) // BATCH_SIZE,\n",
    "    ):\n",
    "        batch_imgs = batch_imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        logits = model(batch_imgs)\n",
    "\n",
    "        loss = loss_fn(sigmoid(logits), labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        predicted_probs = sigmoid(logits).detach().cpu().numpy()\n",
    "        predicted_labels = (predicted_probs > 0.5).astype(float)\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            print(\n",
    "                f\"Train F1-score: {f1_score(labels.detach().cpu().numpy(), predicted_labels):.4f} Train Loss: {loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_imgs, labels in tqdm(\n",
    "            generate_batch(list_val_imgs, BATCH_SIZE), total=len(list_val_imgs) // BATCH_SIZE\n",
    "        ):\n",
    "            batch_imgs = batch_imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            logits = model(batch_imgs)\n",
    "            val_losses.append(loss_fn(sigmoid(logits), labels).detach().cpu().numpy())\n",
    "            predicted_probs = sigmoid(logits).detach().cpu().numpy()\n",
    "            predicted_labels = (predicted_probs > 0.5).astype(float)\n",
    "\n",
    "            val_preds.extend(predicted_labels)\n",
    "            val_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_preds)\n",
    "    print(f\"Validation F1-score: {val_f1:.4f} Validation Loss: {np.mean(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694201f-8181-487b-9cf7-8d7965df87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_preds = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [path_img.split(\".\")[0] for path_img in list_test_imgs],\n",
    "        \"label\": [\n",
    "            sigmoid(\n",
    "                model(\n",
    "                    torch.from_numpy(\n",
    "                        cv2.imread(f\"{PATH_DATA}test1/{path_img}\").astype(np.float32) / 255.0\n",
    "                    ).unsqueeze(0)\n",
    "                )\n",
    "            ).item()\n",
    "            for path_img in list_test_imgs\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100200bb-12a4-4212-9b46-66fac7236929",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41649804-4c4e-467f-974f-b941c63e0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test.to_csv(\"base_from_scratch.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_cv_best11",
   "language": "python",
   "name": "kaggle_cv_best11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
