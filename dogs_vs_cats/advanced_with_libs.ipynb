{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b28949-a345-4a70-bbe4-f593e53e0c2a",
   "metadata": {},
   "source": [
    "#### PyTorch Lightning: для воспроизведения экспериментов\n",
    "#### TensorBoard + Optuna: для логирования экспериментов и подбора гиперпараметров \n",
    "#### Albumentations: для аугментаций\n",
    "#### timm: для использования предобученных моделей для задачи классификации \n",
    "#### captum: для анализа в каких областях изображения у модели наибольшие градиенты, аккумулированные по всем слоям\n",
    "#### ONNX: для инференса "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ca8c2-ae4c-4b60-9332-9dc728998d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations import (\n",
    "    Blur,\n",
    "    CoarseDropout,\n",
    "    Compose,\n",
    "    GaussNoise,\n",
    "    HorizontalFlip,\n",
    "    HueSaturationValue,\n",
    "    MedianBlur,\n",
    "    MotionBlur,\n",
    "    Normalize,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ShiftScaleRotate,\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from captum.attr import IntegratedGradients\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b95a41-a812-4636-9d71-040d7e638d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"/home/a.makarchuk@rit.va/Desktop/kaggle-CV-best11/dogs_vs_cats/data/\"\n",
    "IMAGE_SIZE = (256, 256)\n",
    "IMAGE_SIZE_TEST = (320, 320)\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 1\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_RATIO = 0.9\n",
    "SEED = 42\n",
    "PERCENT = 0.05\n",
    "\n",
    "os.chdir(PATH_DATA)\n",
    "\n",
    "list_train_imgs = os.listdir(f\"{PATH_DATA}train\")\n",
    "list_test_imgs = os.listdir(f\"{PATH_DATA}test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b23b75f-207d-4577-82ab-13fd9a78cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT MEAN AND STD BY %(PERCENT) OF DATA\n",
    "\n",
    "num_imgs = int(len(list_train_imgs) * PERCENT)\n",
    "\n",
    "mean_by_percent = np.mean(\n",
    "    [\n",
    "        np.mean(plt.imread(f\"{PATH_DATA}/train/{path_data}\").astype(float) / 255.0, axis=(0, 1))\n",
    "        for path_data in list_train_imgs[:num_imgs]\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "std_by_percent = np.mean(\n",
    "    [\n",
    "        np.std(plt.imread(f\"{PATH_DATA}/train/{path_data}\").astype(float) / 255.0, axis=(0, 1))\n",
    "        for path_data in list_train_imgs[:num_imgs]\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "print(f\"Mean for {PERCENT*100}% of images: {mean_by_percent}\")\n",
    "print(f\"Std for {PERCENT*100}% of images: {std_by_percent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9466c-7318-4560-bc56-30d52c8b61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE AUGMENTATIONS (tuned)\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        Resize(*IMAGE_SIZE, p=1),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        ShiftScaleRotate(p=0.5),\n",
    "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        CoarseDropout(max_holes=3, max_height=32, max_width=32, p=0.25),\n",
    "        Blur(blur_limit=3, p=0.25),\n",
    "        GaussNoise(p=0.25),\n",
    "        Normalize(mean=mean_by_percent, std=std_by_percent),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        Resize(*IMAGE_SIZE_TEST, p=1),\n",
    "        Normalize(mean=mean_by_percent, std=std_by_percent),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f27c66-63cc-4283-8285-7160d0df3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatsDogsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_imgs, transforms=None):\n",
    "        self.list_imgs = list_imgs\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.list_imgs[index]\n",
    "        img_name = img_path.split(\"/\")[-1]\n",
    "        label = 1 if img_name.split(\".\")[0] == \"dog\" else 0\n",
    "\n",
    "        img = cv2.imread(PATH_DATA + \"train/\" + img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        return img, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_imgs)\n",
    "\n",
    "    def show_augmentated_img(self, index):\n",
    "        img, _ = self[index]\n",
    "        if self.transforms:\n",
    "            img = np.array(img.permute(1, 2, 0), dtype=np.float32)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598c5ac-aee2-4ada-9d17-0712bbf74f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "list_imgs = list_train_imgs[:]\n",
    "random.shuffle(list_imgs)\n",
    "list_train_imgs = list_imgs[: int(len(list_imgs) * TRAIN_RATIO)]\n",
    "list_val_imgs = list_imgs[int(len(list_imgs) * TRAIN_RATIO) :]\n",
    "\n",
    "train_dataset = CatsDogsDataset(list_train_imgs, train_transforms)\n",
    "val_dataset = CatsDogsDataset(list_val_imgs, val_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "plt.imshow(train_dataset.show_augmentated_img(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7799e82-47b7-4c53-90e9-cf598c7f36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_classes=1, pretrained=True, lr=5e-4, thr=0.5):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, num_classes=num_classes, pretrained=pretrained)\n",
    "        self.lr = lr\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.thr = thr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self.model(x).view(-1)\n",
    "        loss = self.loss_fn(y_pred, y_true)\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(loss)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self.model(x).view(-1)\n",
    "        loss = self.loss_fn(y_pred, y_true)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "        preds = (torch.sigmoid(y_pred) > self.thr).float()\n",
    "        f1 = f1_score(y_true.cpu().numpy(), preds.cpu().numpy())\n",
    "        self.log(\"val_f1\", f1)\n",
    "\n",
    "        bce_loss = nn.BCELoss()\n",
    "        self.log(\"val_loss_after_thr\", bce_loss(preds, y_true))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83077b-c353-4287-9af1-53eb69792928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 5e-5, 1e-3, log=True)\n",
    "    thr = trial.suggest_float(\"thr\", 0.3, 0.8)\n",
    "\n",
    "    model = LitModel(\"mobilenetv4_conv_large.e500_r256_in1k\", thr=thr, lr=lr)\n",
    "\n",
    "    logger = TensorBoardLogger(f\"{PATH_DATA}/tb_logs\", name=\"optuna\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "    checkpoint = ModelCheckpoint(monitor=\"val_loss\", save_top_k=1, mode=\"min\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        max_epochs=EPOCHS,\n",
    "        accelerator=\"auto\",\n",
    "        callbacks=[\n",
    "            early_stop,\n",
    "            checkpoint,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss_after_thr\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de0147-a71c-4f0f-b3eb-73290c3d25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3cc3a-fd44-43ce-a028-c5a3ad7be5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e64d90-21a9-42e8-90c5-10d239fe91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir {PATH_DATA}/tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c1475-e2db-4c7c-b8ca-5549e03aac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModel(\n",
    "    \"mobilenetv4_conv_large.e500_r256_in1k\", thr=trial.params[\"thr\"], lr=trial.params[\"lr\"]\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(f\"{PATH_DATA}/tb_logs\", name=\"best\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", save_top_k=1, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    max_epochs=EPOCHS,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[early_stop, checkpoint],\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debbbdd0-815f-4459-bea8-44b60f99ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModel.load_from_checkpoint(\n",
    "    f\"{PATH_DATA}tb_logs/best/version_1/checkpoints/epoch=0-step=2250.ckpt\",\n",
    "    model_name=\"mobilenetv4_conv_large.e500_r256_in1k\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, *(IMAGE_SIZE_TEST))\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\", opset_version=11)\n",
    "\n",
    "ort_session = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "test_preds = []\n",
    "for path_img in tqdm(list_test_imgs):\n",
    "    img = cv2.imread(f\"{PATH_DATA}test1/{path_img}\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = val_transforms(image=img)[\"image\"]\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    input_data = img.numpy()\n",
    "\n",
    "    pred = ort_session.run(None, {input_name: input_data})[0]\n",
    "    pred = torch.sigmoid(torch.from_numpy(pred)).item()\n",
    "\n",
    "    test_preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf794a1-1c9e-43ae-9e65-133d0b2fbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [int(path_img.split(\".\")[0]) for path_img in list_test_imgs],\n",
    "        \"label\": test_preds,\n",
    "    }\n",
    ")\n",
    "\n",
    "test_preds_df = test_preds_df.sort_values(\"id\").reset_index(drop=True)\n",
    "test_preds_df.to_csv(\"advanced_solution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1a265-d8e6-4e76-9562-84bbce9969e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK PREDICTS ON TEST\n",
    "\n",
    "rand_img_train_path = f'{PATH_DATA}test1/{\".\".join(random.choice(list_train_imgs).split(\".\")[1:])}'\n",
    "img = plt.imread(rand_img_train_path)\n",
    "img = cv2.resize(img, IMAGE_SIZE_TEST, interpolation=cv2.INTER_AREA)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(rand_img_train_path)\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "img = val_transforms(image=img)[\"image\"]\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "input_data = img.numpy()\n",
    "\n",
    "pred = ort_session.run(None, {input_name: input_data})[0]\n",
    "\n",
    "sigmoid_output = torch.sigmoid(torch.from_numpy(pred)).item()\n",
    "\n",
    "print(sigmoid_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3c868-267b-4485-85ed-da0ca0b6021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_train_path = f'{PATH_DATA}test1/{\".\".join(random.choice(list_train_imgs).split(\".\")[1:])}'\n",
    "img = plt.imread(rand_img_train_path)\n",
    "img = cv2.resize(img, IMAGE_SIZE_TEST, interpolation=cv2.INTER_AREA)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(rand_img_train_path)\n",
    "\n",
    "img_original = img.copy()\n",
    "\n",
    "img = val_transforms(image=img)[\"image\"]\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "input_data = img.numpy()\n",
    "\n",
    "pred = ort_session.run(None, {input_name: input_data})[0]\n",
    "sigmoid_output = torch.sigmoid(torch.from_numpy(pred)).item()\n",
    "\n",
    "print(sigmoid_output)\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "attributions, delta = ig.attribute(img, target=0, return_convergence_delta=True)\n",
    "\n",
    "attributions = attributions.squeeze().cpu().detach().numpy()\n",
    "attributions = np.transpose(attributions, (1, 2, 0))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.imshow(img_original)\n",
    "ax1.set_title(\"Original Image\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "ax2.imshow(attributions, cmap=\"viridis\", alpha=0.7)\n",
    "ax2.set_title(\"Integrated Gradients\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "plt.colorbar(\n",
    "    ax2.imshow(attributions, cmap=\"viridis\", alpha=0.7), ax=ax2, label=\"Integrated Gradients\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8e161-0388-4fe8-9182-98a01909ccdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_cv_best11",
   "language": "python",
   "name": "kaggle_cv_best11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
